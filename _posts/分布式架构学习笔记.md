

## Scalability

**系统要走向分布式的理由，Scalability 是最常见的理由之一**
简单的将 Scalabilty 的需求可分为：
- Data Scalability: 单台机器的容量不足以存储海量的数据
- Computing Scalability: 单台机器的运算能力不足以及时完成运算


几乎注定要接受一些牺牲：
- 牺牲效率：节点间网络的延迟和协调，必然降低执行效率
- 牺牲运维能力：分布式架构的问题常常很难重现，也很难追踪


跟单机系统一样，也有一些系统设计上的 tradeoffs：
- CPU优先或是IO优化
- 读优化或是写优化
- Throughput优化或是Latency 优化
- 强一致性或者最终一致性

当然选择了不同的 tradeoff，就会有不同的系统架构。不同的设计决
策就会衍生出不同用途的系统，因此也就不存在通用的分布式系统。


## Partition

当数据放不进一台机器，或是对数据的运算太过耗时，单台机器无法负荷时，就是考虑partition的时候。partition 就是把数据切割放到多台机器上，首先要考量的，就是要怎么切分数据。

有些问题是无法避免的：
- 当规模变大的时候，能否通过增加节点来动态适应？
- 当某个节点故障的时候，能否将该节点上的任务均衡的分摊到其他节点？
- 对于可修改的数据（比如数据库数据），如果某节点数据量变大，能否以及如何将部分数据迁移到其他负载较小的节点，及达到动态均衡的效果？
- 元数据的管理（即数据与物理节点的对应关系）规模？元数据更新的频率以及复杂度？

几种常见的切法：
- **Round-Robin**：轮流进多台机器。好处是 load balance，坏处是不适合有 session或数据相关性 (need join) 的应用。变型是可以用thread pool，每个机器固定配几个thread，这可以避免某个运算耗时过久，而阻塞后面运算的问题。
- **Range**：事先定好每台机器的防守范围，如 key 在 1~1000 到 A 机器。优点是简单，只需要维护一些 metadata。问题是弹性较差，且会有 hotspot 的问题 (大量数据可能会集中在某个范围)。


- **Hash**：用 Hash 来决定数据要在哪台机器上。简单的 Hash如取模，但取模在
新增机器时会有数据迁移的问题，所以现在大家比较常用 Consistent Hashing 来避
免这个问题。Hash 可以很平均的分布，且只需要非常少的 metadata。但 Hash 规则
不好掌握，比方说我们就很难透过自定 Hash 规则让某几个数据一定要在一起。大部
分的 Data Store 都是采用 Consistent Hashing。

- **Manual**: 手动建一个对照表，优点是想要怎麽分配都可以，缺点是要自己控制数据和负载的均衡，且会有大量 metadata 要维护。



数据切割是非常应用导向的问题，某个切法可能能让某种运算很有效率，但会害到其他种运算，没有银弹，还是一个tradeoff的问题。事实上 partition 比较常用在 write 需求高的场景。


当系统走向分布式之后所有的机器必须要follow同一套的数据切割方式，这个切割方式存在 metadata中。这个 metadata 如果不见，那之後的数据就不知道该写入哪一台，且每次查询时都必须要广播找资料，这是很不方便的。所以要想办法保存好这些 metadata。有些切割方式，像是 Hashing 的 metadata 量非常少，这是相对容易管的。但有些切割方式有很多 metadata，且有些方式在每次 insert 都要更新 metadata (bad practice~)，那这就麻烦了。

一个最简单的方式就是有一台机器专门管这些 metadata (meta server, config server...)，若需要 metadata 就来这边问。但这又会带来单点的问题。
我们可以用ZK本身的HA和 consistency机制来解决这个问题。


## Replication

